{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.1.0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,2,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4], dtype=torch.int32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "z= torch.from_numpy(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [1,2,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.as_tensor(y).dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 2, 3, 4]), tensor([1, 2, 3, 4], dtype=torch.int32))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[0]=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([100,   2,   3,   4], dtype=torch.int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.7546e+03, 7.8333e-43],\n",
       "        [1.7539e+03, 7.8333e-43]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2., requires_grad=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2*x**3 + x**2 + 4*x +3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(31., grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(32.)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on new data (matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1.,2.,4.],[1,2,3]],requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first layer\n",
    "y = x**2 + x + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.,  7., 21.],\n",
       "        [ 3.,  7., 13.]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# second layer\n",
    "z = y**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  9.,  49., 441.],\n",
       "        [  9.,  49., 169.]], grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outout layer\n",
    "out = z.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(726., grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 18.,  70., 378.],\n",
       "        [ 18.,  70., 182.]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now analyse the data at each layer.\n",
    "# out = sum of all ((x**2 + x + 1)**2)\n",
    "# derivative of out with respect of x is = 2*(x**2 + x + 1)*(2*x+1)\n",
    "# now put first value which is 1, answer would be = 2*(1+1+1)*(2+1) = 18\n",
    "# now put first value which is 2, answer would be = 2*(4+2+1)*(4+1) = 70\n",
    "# now put first value which is 3, answer would be = 2*(16+4+1)*(8+1) = 182\n",
    "# now put first value which is 4, answer would be = 2*(16+4+1)*(8+1) = 378\n",
    "\n",
    "\n",
    "# if mean is there --> then need to take mean , divide the values with total elements in matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0000, 11.6667, 63.0000],\n",
       "        [ 3.0000, 11.6667, 30.3333]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with mean\n",
    "\n",
    "x = torch.tensor([[1.,2.,4.],[1,2,3]],requires_grad=True)\n",
    "# first layer\n",
    "y = x**2 + x + 1\n",
    "# second layer\n",
    "z = y**2\n",
    "# outout layer\n",
    "out = z.mean()\n",
    "\n",
    "out.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  0.,   0., 378.],\n",
       "        [  0.,   0.,   0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with max\n",
    "\n",
    "x = torch.tensor([[1.,2.,4.],[1,2,3]],requires_grad=True)\n",
    "# first layer\n",
    "y = x**2 + x + 1\n",
    "# second layer\n",
    "z = y**2\n",
    "# outout layer\n",
    "out = z.max()\n",
    "\n",
    "out.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 18.,  70., 378.],\n",
       "        [ 18.,  70., 182.]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with sum\n",
    "\n",
    "x = torch.tensor([[1.,2.,4.],[1,2,3]],requires_grad=True)\n",
    "# first layer\n",
    "y = x**2 + x + 1\n",
    "# second layer\n",
    "z = y**2\n",
    "# outout layer\n",
    "out = z.sum()\n",
    "\n",
    "out.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear equations solved through pytorch\n",
    "## step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(71)\n",
    "X = torch.linspace(1,50,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = torch.randint(-7,8,(50,1), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = 2*X +1 + error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 1])"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22f56ab09b0>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(X.numpy(), y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([-0.3306], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[0.0145]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(71)\n",
    "model = nn.Linear(in_features=1, out_features=1)\n",
    "print (model.bias)\n",
    "print (model.weight)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "   <strong>Explanation:: <br><br></strong> \n",
    "    First initializing a model which has its own weight and bias already( without training) depending on the input features and output features given at calling the Model class. \n",
    "    <br><br>\n",
    "    For exmample, model with 2 input and 1 output is initialized.\n",
    "    <br>      \n",
    "    <strong>model = Model(2,1)</strong>\n",
    "    <br><br>\n",
    "    Now will check the model by providing a 'x' random value and analyse the y value with forward function.    \n",
    "    <br><br>\n",
    "    model.forward(x)\n",
    "    \n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial phase . this decides how many input features and out features in model\n",
    "torch.manual_seed(71)\n",
    "\n",
    "model = Model(2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.6639], requires_grad=True)"
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0103, -0.2338]], requires_grad=True)"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight \t\n",
      " Parameter containing:\n",
      "tensor([[ 0.0103, -0.2338]], requires_grad=True) \t\n",
      "\n",
      "linear.bias \t\n",
      " Parameter containing:\n",
      "tensor([0.6639], requires_grad=True) \t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# when model is heavy and more weights and bias\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, '\\t\\n' , param,'\\t\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    weight and bias from modal, equation comes out to be as: <br><br>\n",
    "    y = 0.0103*x1 +(-0.2338)*x2 + 0.6639\n",
    "     <br><br>\n",
    "    now x is provided in model as [1,2]\n",
    "    <br>\n",
    "    then y would be, \n",
    "    <br>\n",
    "    y = 0.0103*1 +(-0.2338)*2 + 0.6639\n",
    "    <br>\n",
    "    y = 0.0103 - 0.4676 + 0.6639 = 0.2066\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## passing the tensor to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.2066], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([1.0,2.0])\n",
    "print( model.forward(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets replicate whole model for 1,1 for simplicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear.weight \t\n",
      " Parameter containing:\n",
      "tensor([[0.0145]], requires_grad=True) \t\n",
      "\n",
      "linear.bias \t\n",
      " Parameter containing:\n",
      "tensor([-0.3306], requires_grad=True) \t\n",
      "\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(71)\n",
    "\n",
    "model = Model(1,1)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, '\\t\\n' , param,'\\t\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(19)\n",
    "x_test = torch.randint(1,50,(10,1), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.forward(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x22f56b92278>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test.numpy(), y_pred.detach().numpy(), 'r')\n",
    "plt.scatter(X.numpy(), y.numpy(), c = 'g')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\">\n",
    "    Above graph looks that model is not able to pick what is actuall bahaviour of the data.\n",
    "       <br><br>\n",
    "        Now, will add the loss function and optimizer into the model, to improve the model efficiency\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(in_features, out_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1    loss: 38419664.00000000  weight: -0.56887245  bias: -0.03454399\n",
      "epoch: 2    loss: 26350020.00000000  weight: 35.75258636  bias: 1.04916406\n",
      "epoch: 3    loss: 18072300.00000000  weight: 65.83212280  bias: 1.94741583\n",
      "epoch: 4    loss: 12395193.00000000  weight: 90.74241638  bias: 2.69208241\n",
      "epoch: 5    loss: 8501664.00000000  weight: 111.37181091  bias: 3.30955768\n",
      "epoch: 6    loss: 5831364.00000000  weight: 128.45599365  bias: 3.82169962\n",
      "epoch: 7    loss: 3999994.25000000  weight: 142.60420227  bias: 4.24660969\n",
      "epoch: 8    loss: 2743985.50000000  weight: 154.32099915  bias: 4.59927893\n",
      "epoch: 9    loss: 1882575.87500000  weight: 164.02423096  bias: 4.89212179\n",
      "epoch: 10   loss: 1291796.12500000  weight: 172.05992126  bias: 5.13541985\n",
      "epoch: 11   loss: 886620.93750000  weight: 178.71464539  bias: 5.33768702\n",
      "epoch: 12   loss: 608740.31250000  weight: 184.22572327  bias: 5.50597477\n",
      "epoch: 13   loss: 418161.06250000  weight: 188.78968811  bias: 5.64612246\n",
      "epoch: 14   loss: 287456.15625000  weight: 192.56930542  bias: 5.76296568\n",
      "epoch: 15   loss: 197814.76562500  weight: 195.69937134  bias: 5.86050940\n",
      "epoch: 16   loss: 136335.92187500  weight: 198.29150391  bias: 5.94207048\n",
      "epoch: 17   loss: 94172.03125000  weight: 200.43814087  bias: 6.01039553\n",
      "epoch: 18   loss: 65254.83984375  weight: 202.21585083  bias: 6.06775904\n",
      "epoch: 19   loss: 45422.14062500  weight: 203.68804932  bias: 6.11604452\n",
      "epoch: 20   loss: 31820.49218750  weight: 204.90721130  bias: 6.15681219\n",
      "epoch: 21   loss: 22491.90625000  weight: 205.91683960  bias: 6.19135427\n",
      "epoch: 22   loss: 16093.97070312  weight: 206.75294495  bias: 6.22074032\n",
      "epoch: 23   loss: 11705.99902344  weight: 207.44534302  bias: 6.24585629\n",
      "epoch: 24   loss: 8696.58105469  weight: 208.01872253  bias: 6.26743603\n",
      "epoch: 25   loss: 6632.55175781  weight: 208.49354553  bias: 6.28608704\n",
      "epoch: 26   loss: 5216.91113281  weight: 208.88674927  bias: 6.30231285\n",
      "epoch: 27   loss: 4245.96093750  weight: 209.21235657  bias: 6.31652975\n",
      "epoch: 28   loss: 3579.99389648  weight: 209.48197937  bias: 6.32908344\n",
      "epoch: 29   loss: 3123.18920898  weight: 209.70524597  bias: 6.34025955\n",
      "epoch: 30   loss: 2809.84008789  weight: 209.89012146  bias: 6.35029459\n",
      "epoch: 31   loss: 2594.86889648  weight: 210.04319763  bias: 6.35938501\n",
      "epoch: 32   loss: 2447.36425781  weight: 210.16995239  bias: 6.36769295\n",
      "epoch: 33   loss: 2346.13208008  weight: 210.27490234  bias: 6.37535286\n",
      "epoch: 34   loss: 2276.65283203  weight: 210.36178589  bias: 6.38247585\n",
      "epoch: 35   loss: 2228.92651367  weight: 210.43371582  bias: 6.38915443\n",
      "epoch: 36   loss: 2196.13964844  weight: 210.49325562  bias: 6.39546490\n",
      "epoch: 37   loss: 2173.59008789  weight: 210.54254150  bias: 6.40147018\n",
      "epoch: 38   loss: 2158.05029297  weight: 210.58334351  bias: 6.40722275\n",
      "epoch: 39   loss: 2147.33471680  weight: 210.61711121  bias: 6.41276646\n",
      "epoch: 40   loss: 2139.91479492  weight: 210.64505005  bias: 6.41813660\n",
      "epoch: 41   loss: 2134.77124023  weight: 210.66816711  bias: 6.42336321\n",
      "epoch: 42   loss: 2131.16772461  weight: 210.68728638  bias: 6.42847109\n",
      "epoch: 43   loss: 2128.63525391  weight: 210.70309448  bias: 6.43348026\n",
      "epoch: 44   loss: 2126.83447266  weight: 210.71615601  bias: 6.43840790\n",
      "epoch: 45   loss: 2125.53466797  weight: 210.72695923  bias: 6.44326782\n",
      "epoch: 46   loss: 2124.58154297  weight: 210.73588562  bias: 6.44807148\n",
      "epoch: 47   loss: 2123.86376953  weight: 210.74325562  bias: 6.45282888\n",
      "epoch: 48   loss: 2123.30883789  weight: 210.74932861  bias: 6.45754766\n",
      "epoch: 49   loss: 2122.84838867  weight: 210.75433350  bias: 6.46223450\n",
      "epoch: 50   loss: 2122.48437500  weight: 210.75845337  bias: 6.46689510\n",
      "epoch: 51   loss: 2122.16455078  weight: 210.76184082  bias: 6.47153378\n",
      "epoch: 52   loss: 2121.88476562  weight: 210.76463318  bias: 6.47615385\n",
      "epoch: 53   loss: 2121.61352539  weight: 210.76692200  bias: 6.48075914\n",
      "epoch: 54   loss: 2121.37792969  weight: 210.76878357  bias: 6.48535156\n",
      "epoch: 55   loss: 2121.15014648  weight: 210.77030945  bias: 6.48993349\n",
      "epoch: 56   loss: 2120.91748047  weight: 210.77154541  bias: 6.49450684\n",
      "epoch: 57   loss: 2120.70751953  weight: 210.77255249  bias: 6.49907303\n",
      "epoch: 58   loss: 2120.49560547  weight: 210.77336121  bias: 6.50363302\n",
      "epoch: 59   loss: 2120.27270508  weight: 210.77400208  bias: 6.50818825\n",
      "epoch: 60   loss: 2120.07153320  weight: 210.77452087  bias: 6.51273918\n",
      "epoch: 61   loss: 2119.86645508  weight: 210.77491760  bias: 6.51728678\n",
      "epoch: 62   loss: 2119.65209961  weight: 210.77522278  bias: 6.52183104\n",
      "epoch: 63   loss: 2119.45092773  weight: 210.77545166  bias: 6.52637291\n",
      "epoch: 64   loss: 2119.24902344  weight: 210.77561951  bias: 6.53091288\n",
      "epoch: 65   loss: 2119.03369141  weight: 210.77574158  bias: 6.53545094\n",
      "epoch: 66   loss: 2118.82983398  weight: 210.77581787  bias: 6.53998756\n",
      "epoch: 67   loss: 2118.63354492  weight: 210.77584839  bias: 6.54452276\n",
      "epoch: 68   loss: 2118.42016602  weight: 210.77586365  bias: 6.54905701\n",
      "epoch: 69   loss: 2118.21240234  weight: 210.77584839  bias: 6.55359030\n",
      "epoch: 70   loss: 2118.01684570  weight: 210.77581787  bias: 6.55812263\n",
      "epoch: 71   loss: 2117.80151367  weight: 210.77575684  bias: 6.56265450\n",
      "epoch: 72   loss: 2117.60083008  weight: 210.77569580  bias: 6.56718540\n",
      "epoch: 73   loss: 2117.38671875  weight: 210.77561951  bias: 6.57171583\n",
      "epoch: 74   loss: 2117.18798828  weight: 210.77552795  bias: 6.57624578\n",
      "epoch: 75   loss: 2116.98583984  weight: 210.77543640  bias: 6.58077526\n",
      "epoch: 76   loss: 2116.77172852  weight: 210.77532959  bias: 6.58530426\n",
      "epoch: 77   loss: 2116.57226562  weight: 210.77522278  bias: 6.58983326\n",
      "epoch: 78   loss: 2116.37011719  weight: 210.77511597  bias: 6.59436178\n",
      "epoch: 79   loss: 2116.15698242  weight: 210.77499390  bias: 6.59888983\n",
      "epoch: 80   loss: 2115.95532227  weight: 210.77487183  bias: 6.60341740\n",
      "epoch: 81   loss: 2115.75830078  weight: 210.77474976  bias: 6.60794497\n",
      "epoch: 82   loss: 2115.54467773  weight: 210.77462769  bias: 6.61247206\n",
      "epoch: 83   loss: 2115.34448242  weight: 210.77450562  bias: 6.61699915\n",
      "epoch: 84   loss: 2115.12915039  weight: 210.77436829  bias: 6.62152576\n",
      "epoch: 85   loss: 2114.93237305  weight: 210.77424622  bias: 6.62605238\n",
      "epoch: 86   loss: 2114.73120117  weight: 210.77410889  bias: 6.63057852\n",
      "epoch: 87   loss: 2114.51660156  weight: 210.77397156  bias: 6.63510466\n",
      "epoch: 88   loss: 2114.31518555  weight: 210.77384949  bias: 6.63963032\n",
      "epoch: 89   loss: 2114.12084961  weight: 210.77371216  bias: 6.64415598\n",
      "epoch: 90   loss: 2113.90454102  weight: 210.77357483  bias: 6.64868116\n",
      "epoch: 91   loss: 2113.69970703  weight: 210.77343750  bias: 6.65320635\n",
      "epoch: 92   loss: 2113.50463867  weight: 210.77330017  bias: 6.65773106\n",
      "epoch: 93   loss: 2113.28857422  weight: 210.77316284  bias: 6.66225576\n",
      "epoch: 94   loss: 2113.08911133  weight: 210.77304077  bias: 6.66677999\n",
      "epoch: 95   loss: 2112.87353516  weight: 210.77290344  bias: 6.67130423\n",
      "epoch: 96   loss: 2112.67822266  weight: 210.77276611  bias: 6.67582798\n",
      "epoch: 97   loss: 2112.47729492  weight: 210.77262878  bias: 6.68035173\n",
      "epoch: 98   loss: 2112.26147461  weight: 210.77249146  bias: 6.68487549\n",
      "epoch: 99   loss: 2112.06079102  weight: 210.77236938  bias: 6.68939877\n",
      "epoch: 100  loss: 2111.86474609  weight: 210.77223206  bias: 6.69392157\n",
      "epoch: 101  loss: 2111.65087891  weight: 210.77209473  bias: 6.69844437\n",
      "epoch: 102  loss: 2111.45214844  weight: 210.77195740  bias: 6.70296717\n",
      "epoch: 103  loss: 2111.23413086  weight: 210.77182007  bias: 6.70748949\n",
      "epoch: 104  loss: 2111.04028320  weight: 210.77169800  bias: 6.71201181\n",
      "epoch: 105  loss: 2110.84106445  weight: 210.77156067  bias: 6.71653366\n",
      "epoch: 106  loss: 2110.62426758  weight: 210.77142334  bias: 6.72105551\n",
      "epoch: 107  loss: 2110.42480469  weight: 210.77128601  bias: 6.72557688\n",
      "epoch: 108  loss: 2110.23071289  weight: 210.77114868  bias: 6.73009825\n",
      "epoch: 109  loss: 2110.01074219  weight: 210.77101135  bias: 6.73461914\n",
      "epoch: 110  loss: 2109.80981445  weight: 210.77088928  bias: 6.73914003\n",
      "epoch: 111  loss: 2109.61352539  weight: 210.77075195  bias: 6.74366045\n",
      "epoch: 112  loss: 2109.39843750  weight: 210.77061462  bias: 6.74818087\n",
      "epoch: 113  loss: 2109.19799805  weight: 210.77049255  bias: 6.75270128\n",
      "epoch: 114  loss: 2108.98437500  weight: 210.77035522  bias: 6.75722122\n",
      "epoch: 115  loss: 2108.78857422  weight: 210.77021790  bias: 6.76174116\n",
      "epoch: 116  loss: 2108.58935547  weight: 210.77008057  bias: 6.76626062\n",
      "epoch: 117  loss: 2108.37475586  weight: 210.76994324  bias: 6.77078009\n",
      "epoch: 118  loss: 2108.17553711  weight: 210.76980591  bias: 6.77529907\n",
      "epoch: 119  loss: 2107.97875977  weight: 210.76966858  bias: 6.77981806\n",
      "epoch: 120  loss: 2107.76489258  weight: 210.76953125  bias: 6.78433657\n",
      "epoch: 121  loss: 2107.56494141  weight: 210.76940918  bias: 6.78885508\n",
      "epoch: 122  loss: 2107.35253906  weight: 210.76927185  bias: 6.79337311\n",
      "epoch: 123  loss: 2107.15502930  weight: 210.76914978  bias: 6.79789114\n",
      "epoch: 124  loss: 2106.95458984  weight: 210.76901245  bias: 6.80240870\n",
      "epoch: 125  loss: 2106.73974609  weight: 210.76887512  bias: 6.80692625\n",
      "epoch: 126  loss: 2106.53930664  weight: 210.76873779  bias: 6.81144381\n",
      "epoch: 127  loss: 2106.34375000  weight: 210.76860046  bias: 6.81596088\n",
      "epoch: 128  loss: 2106.13085938  weight: 210.76846313  bias: 6.82047796\n",
      "epoch: 129  loss: 2105.93090820  weight: 210.76834106  bias: 6.82499456\n",
      "epoch: 130  loss: 2105.71582031  weight: 210.76820374  bias: 6.82951117\n",
      "epoch: 131  loss: 2105.52124023  weight: 210.76806641  bias: 6.83402729\n",
      "epoch: 132  loss: 2105.32250977  weight: 210.76792908  bias: 6.83854342\n",
      "epoch: 133  loss: 2105.10766602  weight: 210.76779175  bias: 6.84305906\n",
      "epoch: 134  loss: 2104.90747070  weight: 210.76766968  bias: 6.84757471\n",
      "epoch: 135  loss: 2104.71386719  weight: 210.76753235  bias: 6.85208988\n",
      "epoch: 136  loss: 2104.49902344  weight: 210.76739502  bias: 6.85660505\n",
      "epoch: 137  loss: 2104.29931641  weight: 210.76725769  bias: 6.86112022\n",
      "epoch: 138  loss: 2104.08422852  weight: 210.76712036  bias: 6.86563492\n",
      "epoch: 139  loss: 2103.88842773  weight: 210.76699829  bias: 6.87014961\n",
      "epoch: 140  loss: 2103.68701172  weight: 210.76686096  bias: 6.87466383\n",
      "epoch: 141  loss: 2103.47534180  weight: 210.76672363  bias: 6.87917805\n",
      "epoch: 142  loss: 2103.27343750  weight: 210.76658630  bias: 6.88369179\n",
      "epoch: 143  loss: 2103.08056641  weight: 210.76646423  bias: 6.88820553\n",
      "epoch: 144  loss: 2102.86523438  weight: 210.76632690  bias: 6.89271879\n",
      "epoch: 145  loss: 2102.66674805  weight: 210.76618958  bias: 6.89723206\n",
      "epoch: 146  loss: 2102.45214844  weight: 210.76605225  bias: 6.90174484\n",
      "epoch: 147  loss: 2102.25732422  weight: 210.76593018  bias: 6.90625763\n",
      "epoch: 148  loss: 2102.05664062  weight: 210.76579285  bias: 6.91076994\n",
      "epoch: 149  loss: 2101.84667969  weight: 210.76565552  bias: 6.91528225\n",
      "epoch: 150  loss: 2101.64819336  weight: 210.76553345  bias: 6.91979456\n",
      "epoch: 151  loss: 2101.43359375  weight: 210.76539612  bias: 6.92430639\n",
      "epoch: 152  loss: 2101.23950195  weight: 210.76525879  bias: 6.92881823\n",
      "epoch: 153  loss: 2101.04125977  weight: 210.76512146  bias: 6.93332958\n",
      "epoch: 154  loss: 2100.82470703  weight: 210.76498413  bias: 6.93784094\n",
      "epoch: 155  loss: 2100.62646484  weight: 210.76484680  bias: 6.94235182\n",
      "epoch: 156  loss: 2100.43261719  weight: 210.76470947  bias: 6.94686270\n",
      "epoch: 157  loss: 2100.21606445  weight: 210.76457214  bias: 6.95137310\n",
      "epoch: 158  loss: 2100.01782227  weight: 210.76445007  bias: 6.95588350\n",
      "epoch: 159  loss: 2099.80517578  weight: 210.76431274  bias: 6.96039343\n",
      "epoch: 160  loss: 2099.60937500  weight: 210.76417542  bias: 6.96490335\n",
      "epoch: 161  loss: 2099.41088867  weight: 210.76403809  bias: 6.96941328\n",
      "epoch: 162  loss: 2099.19677734  weight: 210.76391602  bias: 6.97392273\n",
      "epoch: 163  loss: 2098.99609375  weight: 210.76377869  bias: 6.97843218\n",
      "epoch: 164  loss: 2098.80419922  weight: 210.76364136  bias: 6.98294115\n",
      "epoch: 165  loss: 2098.58959961  weight: 210.76350403  bias: 6.98745012\n",
      "epoch: 166  loss: 2098.38842773  weight: 210.76338196  bias: 6.99195862\n",
      "epoch: 167  loss: 2098.17578125  weight: 210.76324463  bias: 6.99646711\n",
      "epoch: 168  loss: 2097.98071289  weight: 210.76310730  bias: 7.00097513\n",
      "epoch: 169  loss: 2097.78735352  weight: 210.76296997  bias: 7.00548315\n",
      "epoch: 170  loss: 2097.57226562  weight: 210.76283264  bias: 7.00999069\n",
      "epoch: 171  loss: 2097.37500000  weight: 210.76271057  bias: 7.01449823\n",
      "epoch: 172  loss: 2097.15966797  weight: 210.76257324  bias: 7.01900530\n",
      "epoch: 173  loss: 2096.96728516  weight: 210.76243591  bias: 7.02351236\n",
      "epoch: 174  loss: 2096.76586914  weight: 210.76229858  bias: 7.02801943\n",
      "epoch: 175  loss: 2096.55346680  weight: 210.76216125  bias: 7.03252602\n",
      "epoch: 176  loss: 2096.35302734  weight: 210.76203918  bias: 7.03703260\n",
      "epoch: 177  loss: 2096.15844727  weight: 210.76190186  bias: 7.04153872\n",
      "epoch: 178  loss: 2095.94433594  weight: 210.76176453  bias: 7.04604483\n",
      "epoch: 179  loss: 2095.74536133  weight: 210.76164246  bias: 7.05055046\n",
      "epoch: 180  loss: 2095.53295898  weight: 210.76150513  bias: 7.05505610\n",
      "epoch: 181  loss: 2095.33740234  weight: 210.76136780  bias: 7.05956125\n",
      "epoch: 182  loss: 2095.14428711  weight: 210.76123047  bias: 7.06406641\n",
      "epoch: 183  loss: 2094.93164062  weight: 210.76109314  bias: 7.06857109\n",
      "epoch: 184  loss: 2094.73217773  weight: 210.76097107  bias: 7.07307577\n",
      "epoch: 185  loss: 2094.51782227  weight: 210.76083374  bias: 7.07757998\n",
      "epoch: 186  loss: 2094.32666016  weight: 210.76069641  bias: 7.08208418\n",
      "epoch: 187  loss: 2094.12646484  weight: 210.76055908  bias: 7.08658838\n",
      "epoch: 188  loss: 2093.91357422  weight: 210.76042175  bias: 7.09109211\n",
      "epoch: 189  loss: 2093.71313477  weight: 210.76029968  bias: 7.09559584\n",
      "epoch: 190  loss: 2093.51782227  weight: 210.76016235  bias: 7.10009909\n",
      "epoch: 191  loss: 2093.30786133  weight: 210.76002502  bias: 7.10460234\n",
      "epoch: 192  loss: 2093.10986328  weight: 210.75990295  bias: 7.10910511\n",
      "epoch: 193  loss: 2092.89746094  weight: 210.75976562  bias: 7.11360788\n",
      "epoch: 194  loss: 2092.69848633  weight: 210.75962830  bias: 7.11811018\n",
      "epoch: 195  loss: 2092.50463867  weight: 210.75949097  bias: 7.12261248\n",
      "epoch: 196  loss: 2092.29223633  weight: 210.75935364  bias: 7.12711477\n",
      "epoch: 197  loss: 2092.09277344  weight: 210.75923157  bias: 7.13161659\n",
      "epoch: 198  loss: 2091.87963867  weight: 210.75909424  bias: 7.13611794\n",
      "epoch: 199  loss: 2091.68481445  weight: 210.75895691  bias: 7.14061928\n",
      "epoch: 200  loss: 2091.48950195  weight: 210.75881958  bias: 7.14512062\n",
      "epoch: 201  loss: 2091.27758789  weight: 210.75869751  bias: 7.14962149\n",
      "epoch: 202  loss: 2091.07788086  weight: 210.75856018  bias: 7.15412235\n",
      "epoch: 203  loss: 2090.86718750  weight: 210.75842285  bias: 7.15862274\n",
      "epoch: 204  loss: 2090.67260742  weight: 210.75830078  bias: 7.16312313\n",
      "epoch: 205  loss: 2090.47485352  weight: 210.75816345  bias: 7.16762304\n",
      "epoch: 206  loss: 2090.26074219  weight: 210.75802612  bias: 7.17212296\n",
      "epoch: 207  loss: 2090.06787109  weight: 210.75788879  bias: 7.17662239\n",
      "epoch: 208  loss: 2089.85229492  weight: 210.75775146  bias: 7.18112183\n",
      "epoch: 209  loss: 2089.66040039  weight: 210.75762939  bias: 7.18562126\n",
      "epoch: 210  loss: 2089.46264648  weight: 210.75749207  bias: 7.19012022\n",
      "epoch: 211  loss: 2089.24902344  weight: 210.75735474  bias: 7.19461918\n",
      "epoch: 212  loss: 2089.04980469  weight: 210.75721741  bias: 7.19911766\n",
      "epoch: 213  loss: 2088.85473633  weight: 210.75708008  bias: 7.20361614\n",
      "epoch: 214  loss: 2088.64599609  weight: 210.75694275  bias: 7.20811415\n",
      "epoch: 215  loss: 2088.44677734  weight: 210.75682068  bias: 7.21261215\n",
      "epoch: 216  loss: 2088.23486328  weight: 210.75668335  bias: 7.21710968\n",
      "epoch: 217  loss: 2088.03710938  weight: 210.75656128  bias: 7.22160721\n",
      "epoch: 218  loss: 2087.84472656  weight: 210.75642395  bias: 7.22610426\n",
      "epoch: 219  loss: 2087.63110352  weight: 210.75628662  bias: 7.23060131\n",
      "epoch: 220  loss: 2087.43798828  weight: 210.75614929  bias: 7.23509836\n",
      "epoch: 221  loss: 2087.22460938  weight: 210.75601196  bias: 7.23959494\n",
      "epoch: 222  loss: 2087.02368164  weight: 210.75588989  bias: 7.24409151\n",
      "epoch: 223  loss: 2086.83422852  weight: 210.75575256  bias: 7.24858761\n",
      "epoch: 224  loss: 2086.62036133  weight: 210.75561523  bias: 7.25308371\n",
      "epoch: 225  loss: 2086.42138672  weight: 210.75549316  bias: 7.25757933\n",
      "epoch: 226  loss: 2086.21484375  weight: 210.75535583  bias: 7.26207495\n",
      "epoch: 227  loss: 2086.01318359  weight: 210.75521851  bias: 7.26657009\n",
      "epoch: 228  loss: 2085.82055664  weight: 210.75508118  bias: 7.27106524\n",
      "epoch: 229  loss: 2085.61010742  weight: 210.75494385  bias: 7.27555990\n",
      "epoch: 230  loss: 2085.40942383  weight: 210.75482178  bias: 7.28005457\n",
      "epoch: 231  loss: 2085.19726562  weight: 210.75468445  bias: 7.28454876\n",
      "epoch: 232  loss: 2085.00488281  weight: 210.75454712  bias: 7.28904295\n",
      "epoch: 233  loss: 2084.80834961  weight: 210.75440979  bias: 7.29353714\n",
      "epoch: 234  loss: 2084.59838867  weight: 210.75427246  bias: 7.29803085\n",
      "epoch: 235  loss: 2084.39916992  weight: 210.75415039  bias: 7.30252457\n",
      "epoch: 236  loss: 2084.18603516  weight: 210.75401306  bias: 7.30701780\n",
      "epoch: 237  loss: 2083.99169922  weight: 210.75389099  bias: 7.31151104\n",
      "epoch: 238  loss: 2083.79907227  weight: 210.75375366  bias: 7.31600380\n",
      "epoch: 239  loss: 2083.58544922  weight: 210.75361633  bias: 7.32049656\n",
      "epoch: 240  loss: 2083.39038086  weight: 210.75349426  bias: 7.32498884\n",
      "epoch: 241  loss: 2083.17749023  weight: 210.75335693  bias: 7.32948112\n",
      "epoch: 242  loss: 2082.98461914  weight: 210.75321960  bias: 7.33397293\n",
      "epoch: 243  loss: 2082.79052734  weight: 210.75308228  bias: 7.33846474\n",
      "epoch: 244  loss: 2082.58056641  weight: 210.75294495  bias: 7.34295654\n",
      "epoch: 245  loss: 2082.38159180  weight: 210.75282288  bias: 7.34744787\n",
      "epoch: 246  loss: 2082.16821289  weight: 210.75268555  bias: 7.35193872\n",
      "epoch: 247  loss: 2081.97363281  weight: 210.75254822  bias: 7.35642958\n",
      "epoch: 248  loss: 2081.78002930  weight: 210.75241089  bias: 7.36092043\n",
      "epoch: 249  loss: 2081.56811523  weight: 210.75227356  bias: 7.36541080\n",
      "epoch: 250  loss: 2081.37133789  weight: 210.75215149  bias: 7.36990118\n",
      "epoch: 251  loss: 2081.15917969  weight: 210.75201416  bias: 7.37439108\n",
      "epoch: 252  loss: 2080.96313477  weight: 210.75187683  bias: 7.37888098\n",
      "epoch: 253  loss: 2080.77099609  weight: 210.75175476  bias: 7.38337040\n",
      "epoch: 254  loss: 2080.56127930  weight: 210.75161743  bias: 7.38785982\n",
      "epoch: 255  loss: 2080.36035156  weight: 210.75148010  bias: 7.39234924\n",
      "epoch: 256  loss: 2080.15405273  weight: 210.75134277  bias: 7.39683819\n",
      "epoch: 257  loss: 2079.95507812  weight: 210.75122070  bias: 7.40132713\n",
      "epoch: 258  loss: 2079.76245117  weight: 210.75108337  bias: 7.40581560\n",
      "epoch: 259  loss: 2079.55273438  weight: 210.75094604  bias: 7.41030407\n",
      "epoch: 260  loss: 2079.35815430  weight: 210.75080872  bias: 7.41479206\n",
      "epoch: 261  loss: 2079.14453125  weight: 210.75068665  bias: 7.41928005\n",
      "epoch: 262  loss: 2078.94702148  weight: 210.75054932  bias: 7.42376757\n",
      "epoch: 263  loss: 2078.75463867  weight: 210.75041199  bias: 7.42825508\n",
      "epoch: 264  loss: 2078.54370117  weight: 210.75027466  bias: 7.43274212\n",
      "epoch: 265  loss: 2078.34985352  weight: 210.75015259  bias: 7.43722916\n",
      "epoch: 266  loss: 2078.13891602  weight: 210.75001526  bias: 7.44171572\n",
      "epoch: 267  loss: 2077.94042969  weight: 210.74987793  bias: 7.44620228\n",
      "epoch: 268  loss: 2077.74926758  weight: 210.74974060  bias: 7.45068884\n",
      "epoch: 269  loss: 2077.54003906  weight: 210.74960327  bias: 7.45517492\n",
      "epoch: 270  loss: 2077.34106445  weight: 210.74948120  bias: 7.45966101\n",
      "epoch: 271  loss: 2077.12963867  weight: 210.74934387  bias: 7.46414661\n",
      "epoch: 272  loss: 2076.93261719  weight: 210.74922180  bias: 7.46863222\n",
      "epoch: 273  loss: 2076.72607422  weight: 210.74908447  bias: 7.47311735\n",
      "epoch: 274  loss: 2076.53051758  weight: 210.74894714  bias: 7.47760248\n",
      "epoch: 275  loss: 2076.33496094  weight: 210.74882507  bias: 7.48208714\n",
      "epoch: 276  loss: 2076.12939453  weight: 210.74868774  bias: 7.48657179\n",
      "epoch: 277  loss: 2075.92846680  weight: 210.74855042  bias: 7.49105597\n",
      "epoch: 278  loss: 2075.71923828  weight: 210.74841309  bias: 7.49554014\n",
      "epoch: 279  loss: 2075.52563477  weight: 210.74829102  bias: 7.50002432\n",
      "epoch: 280  loss: 2075.33154297  weight: 210.74815369  bias: 7.50450802\n",
      "epoch: 281  loss: 2075.12207031  weight: 210.74801636  bias: 7.50899172\n",
      "epoch: 282  loss: 2074.92480469  weight: 210.74787903  bias: 7.51347494\n",
      "epoch: 283  loss: 2074.71215820  weight: 210.74774170  bias: 7.51795816\n",
      "epoch: 284  loss: 2074.51879883  weight: 210.74761963  bias: 7.52244091\n",
      "epoch: 285  loss: 2074.32543945  weight: 210.74748230  bias: 7.52692366\n",
      "epoch: 286  loss: 2074.11596680  weight: 210.74734497  bias: 7.53140593\n",
      "epoch: 287  loss: 2073.92260742  weight: 210.74722290  bias: 7.53588820\n",
      "epoch: 288  loss: 2073.71215820  weight: 210.74708557  bias: 7.54036999\n",
      "epoch: 289  loss: 2073.51367188  weight: 210.74694824  bias: 7.54485178\n",
      "epoch: 290  loss: 2073.32275391  weight: 210.74681091  bias: 7.54933357\n",
      "epoch: 291  loss: 2073.11401367  weight: 210.74667358  bias: 7.55381489\n",
      "epoch: 292  loss: 2072.91528320  weight: 210.74655151  bias: 7.55829620\n",
      "epoch: 293  loss: 2072.70410156  weight: 210.74641418  bias: 7.56277704\n",
      "epoch: 294  loss: 2072.51245117  weight: 210.74629211  bias: 7.56725788\n",
      "epoch: 295  loss: 2072.30151367  weight: 210.74615479  bias: 7.57173824\n",
      "epoch: 296  loss: 2072.10742188  weight: 210.74601746  bias: 7.57621861\n",
      "epoch: 297  loss: 2071.91064453  weight: 210.74589539  bias: 7.58069849\n",
      "epoch: 298  loss: 2071.70483398  weight: 210.74575806  bias: 7.58517838\n",
      "epoch: 299  loss: 2071.50610352  weight: 210.74562073  bias: 7.58965778\n",
      "epoch: 300  loss: 2071.29589844  weight: 210.74548340  bias: 7.59413719\n",
      "epoch: 301  loss: 2071.10205078  weight: 210.74536133  bias: 7.59861660\n",
      "epoch: 302  loss: 2070.90869141  weight: 210.74522400  bias: 7.60309553\n",
      "epoch: 303  loss: 2070.70117188  weight: 210.74508667  bias: 7.60757446\n",
      "epoch: 304  loss: 2070.50585938  weight: 210.74496460  bias: 7.61205292\n",
      "epoch: 305  loss: 2070.29394531  weight: 210.74482727  bias: 7.61653137\n",
      "epoch: 306  loss: 2070.09863281  weight: 210.74468994  bias: 7.62100935\n",
      "epoch: 307  loss: 2069.89135742  weight: 210.74455261  bias: 7.62548733\n",
      "epoch: 308  loss: 2069.70043945  weight: 210.74443054  bias: 7.62996483\n",
      "epoch: 309  loss: 2069.50341797  weight: 210.74429321  bias: 7.63444233\n",
      "epoch: 310  loss: 2069.29296875  weight: 210.74415588  bias: 7.63891935\n",
      "epoch: 311  loss: 2069.09863281  weight: 210.74401855  bias: 7.64339638\n",
      "epoch: 312  loss: 2068.89013672  weight: 210.74388123  bias: 7.64787340\n",
      "epoch: 313  loss: 2068.69531250  weight: 210.74375916  bias: 7.65234995\n",
      "epoch: 314  loss: 2068.50244141  weight: 210.74362183  bias: 7.65682650\n",
      "epoch: 315  loss: 2068.29125977  weight: 210.74348450  bias: 7.66130257\n",
      "epoch: 316  loss: 2068.09521484  weight: 210.74336243  bias: 7.66577864\n",
      "epoch: 317  loss: 2067.88940430  weight: 210.74322510  bias: 7.67025423\n",
      "epoch: 318  loss: 2067.69042969  weight: 210.74308777  bias: 7.67472982\n",
      "epoch: 319  loss: 2067.49926758  weight: 210.74296570  bias: 7.67920494\n",
      "epoch: 320  loss: 2067.29418945  weight: 210.74282837  bias: 7.68368006\n",
      "epoch: 321  loss: 2067.09497070  weight: 210.74269104  bias: 7.68815470\n",
      "epoch: 322  loss: 2066.88525391  weight: 210.74255371  bias: 7.69262934\n",
      "epoch: 323  loss: 2066.69189453  weight: 210.74243164  bias: 7.69710398\n",
      "epoch: 324  loss: 2066.48120117  weight: 210.74229431  bias: 7.70157814\n",
      "epoch: 325  loss: 2066.28808594  weight: 210.74217224  bias: 7.70605230\n",
      "epoch: 326  loss: 2066.09667969  weight: 210.74203491  bias: 7.71052599\n",
      "epoch: 327  loss: 2065.88598633  weight: 210.74189758  bias: 7.71499968\n",
      "epoch: 328  loss: 2065.69506836  weight: 210.74177551  bias: 7.71947289\n",
      "epoch: 329  loss: 2065.48339844  weight: 210.74163818  bias: 7.72394609\n",
      "epoch: 330  loss: 2065.28857422  weight: 210.74150085  bias: 7.72841883\n",
      "epoch: 331  loss: 2065.08129883  weight: 210.74136353  bias: 7.73289156\n",
      "epoch: 332  loss: 2064.88867188  weight: 210.74124146  bias: 7.73736382\n",
      "epoch: 333  loss: 2064.69384766  weight: 210.74110413  bias: 7.74183607\n",
      "epoch: 334  loss: 2064.48632812  weight: 210.74096680  bias: 7.74630785\n",
      "epoch: 335  loss: 2064.29028320  weight: 210.74084473  bias: 7.75077963\n",
      "epoch: 336  loss: 2064.08032227  weight: 210.74070740  bias: 7.75525093\n",
      "epoch: 337  loss: 2063.88500977  weight: 210.74057007  bias: 7.75972223\n",
      "epoch: 338  loss: 2063.69531250  weight: 210.74043274  bias: 7.76419353\n",
      "epoch: 339  loss: 2063.48999023  weight: 210.74029541  bias: 7.76866436\n",
      "epoch: 340  loss: 2063.29028320  weight: 210.74017334  bias: 7.77313519\n",
      "epoch: 341  loss: 2063.08203125  weight: 210.74003601  bias: 7.77760553\n",
      "epoch: 342  loss: 2062.89062500  weight: 210.73989868  bias: 7.78207588\n",
      "epoch: 343  loss: 2062.67797852  weight: 210.73976135  bias: 7.78654575\n",
      "epoch: 344  loss: 2062.48754883  weight: 210.73963928  bias: 7.79101562\n",
      "epoch: 345  loss: 2062.29443359  weight: 210.73950195  bias: 7.79548550\n",
      "epoch: 346  loss: 2062.08398438  weight: 210.73936462  bias: 7.79995489\n",
      "epoch: 347  loss: 2061.89111328  weight: 210.73924255  bias: 7.80442429\n",
      "epoch: 348  loss: 2061.68286133  weight: 210.73910522  bias: 7.80889320\n",
      "epoch: 349  loss: 2061.48535156  weight: 210.73898315  bias: 7.81336212\n",
      "epoch: 350  loss: 2061.28027344  weight: 210.73884583  bias: 7.81783056\n",
      "epoch: 351  loss: 2061.08911133  weight: 210.73870850  bias: 7.82229900\n",
      "epoch: 352  loss: 2060.89794922  weight: 210.73857117  bias: 7.82676697\n",
      "epoch: 353  loss: 2060.68774414  weight: 210.73843384  bias: 7.83123493\n",
      "epoch: 354  loss: 2060.48876953  weight: 210.73831177  bias: 7.83570290\n",
      "epoch: 355  loss: 2060.28540039  weight: 210.73817444  bias: 7.84017038\n",
      "epoch: 356  loss: 2060.08740234  weight: 210.73805237  bias: 7.84463787\n",
      "epoch: 357  loss: 2059.88208008  weight: 210.73791504  bias: 7.84910488\n",
      "epoch: 358  loss: 2059.69189453  weight: 210.73779297  bias: 7.85357189\n",
      "epoch: 359  loss: 2059.49536133  weight: 210.73765564  bias: 7.85803843\n",
      "epoch: 360  loss: 2059.28833008  weight: 210.73751831  bias: 7.86250496\n",
      "epoch: 361  loss: 2059.09448242  weight: 210.73738098  bias: 7.86697102\n",
      "epoch: 362  loss: 2058.88549805  weight: 210.73724365  bias: 7.87143707\n",
      "epoch: 363  loss: 2058.69213867  weight: 210.73712158  bias: 7.87590265\n",
      "epoch: 364  loss: 2058.48608398  weight: 210.73698425  bias: 7.88036823\n",
      "epoch: 365  loss: 2058.29321289  weight: 210.73686218  bias: 7.88483334\n",
      "epoch: 366  loss: 2058.09692383  weight: 210.73672485  bias: 7.88929844\n",
      "epoch: 367  loss: 2057.88940430  weight: 210.73658752  bias: 7.89376307\n",
      "epoch: 368  loss: 2057.69555664  weight: 210.73646545  bias: 7.89822769\n",
      "epoch: 369  loss: 2057.48999023  weight: 210.73632812  bias: 7.90269184\n",
      "epoch: 370  loss: 2057.29516602  weight: 210.73619080  bias: 7.90715599\n",
      "epoch: 371  loss: 2057.08789062  weight: 210.73605347  bias: 7.91162014\n",
      "epoch: 372  loss: 2056.89721680  weight: 210.73593140  bias: 7.91608381\n",
      "epoch: 373  loss: 2056.70092773  weight: 210.73579407  bias: 7.92054749\n",
      "epoch: 374  loss: 2056.49511719  weight: 210.73565674  bias: 7.92501068\n",
      "epoch: 375  loss: 2056.29907227  weight: 210.73553467  bias: 7.92947388\n",
      "epoch: 376  loss: 2056.09228516  weight: 210.73539734  bias: 7.93393660\n",
      "epoch: 377  loss: 2055.89843750  weight: 210.73526001  bias: 7.93839931\n",
      "epoch: 378  loss: 2055.69189453  weight: 210.73512268  bias: 7.94286156\n",
      "epoch: 379  loss: 2055.50146484  weight: 210.73500061  bias: 7.94732380\n",
      "epoch: 380  loss: 2055.30517578  weight: 210.73486328  bias: 7.95178604\n",
      "epoch: 381  loss: 2055.10058594  weight: 210.73472595  bias: 7.95624781\n",
      "epoch: 382  loss: 2054.90551758  weight: 210.73460388  bias: 7.96070957\n",
      "epoch: 383  loss: 2054.69921875  weight: 210.73446655  bias: 7.96517086\n",
      "epoch: 384  loss: 2054.50537109  weight: 210.73432922  bias: 7.96963215\n",
      "epoch: 385  loss: 2054.29858398  weight: 210.73419189  bias: 7.97409296\n",
      "epoch: 386  loss: 2054.10742188  weight: 210.73406982  bias: 7.97855377\n",
      "epoch: 387  loss: 2053.91577148  weight: 210.73393250  bias: 7.98301411\n",
      "epoch: 388  loss: 2053.70581055  weight: 210.73379517  bias: 7.98747444\n",
      "epoch: 389  loss: 2053.51538086  weight: 210.73367310  bias: 7.99193478\n",
      "epoch: 390  loss: 2053.30541992  weight: 210.73353577  bias: 7.99639463\n",
      "epoch: 391  loss: 2053.11572266  weight: 210.73341370  bias: 8.00085449\n",
      "epoch: 392  loss: 2052.90332031  weight: 210.73327637  bias: 8.00531387\n",
      "epoch: 393  loss: 2052.71508789  weight: 210.73313904  bias: 8.00977325\n",
      "epoch: 394  loss: 2052.52172852  weight: 210.73300171  bias: 8.01423264\n",
      "epoch: 395  loss: 2052.31201172  weight: 210.73287964  bias: 8.01869106\n",
      "epoch: 396  loss: 2052.11938477  weight: 210.73274231  bias: 8.02314949\n",
      "epoch: 397  loss: 2051.91064453  weight: 210.73260498  bias: 8.02760792\n",
      "epoch: 398  loss: 2051.72119141  weight: 210.73248291  bias: 8.03206635\n",
      "epoch: 399  loss: 2051.50976562  weight: 210.73234558  bias: 8.03652382\n",
      "epoch: 400  loss: 2051.31933594  weight: 210.73222351  bias: 8.04098129\n",
      "epoch: 401  loss: 2051.13061523  weight: 210.73208618  bias: 8.04543877\n",
      "epoch: 402  loss: 2050.92211914  weight: 210.73194885  bias: 8.04989624\n",
      "epoch: 403  loss: 2050.72827148  weight: 210.73181152  bias: 8.05435371\n",
      "epoch: 404  loss: 2050.52221680  weight: 210.73168945  bias: 8.05881023\n",
      "epoch: 405  loss: 2050.32690430  weight: 210.73155212  bias: 8.06326675\n",
      "epoch: 406  loss: 2050.12402344  weight: 210.73141479  bias: 8.06772327\n",
      "epoch: 407  loss: 2049.92700195  weight: 210.73129272  bias: 8.07217979\n",
      "epoch: 408  loss: 2049.72094727  weight: 210.73115540  bias: 8.07663536\n",
      "epoch: 409  loss: 2049.53271484  weight: 210.73103333  bias: 8.08109093\n",
      "epoch: 410  loss: 2049.34057617  weight: 210.73089600  bias: 8.08554649\n",
      "epoch: 411  loss: 2049.13305664  weight: 210.73075867  bias: 8.09000206\n",
      "epoch: 412  loss: 2048.94067383  weight: 210.73062134  bias: 8.09445763\n",
      "epoch: 413  loss: 2048.73022461  weight: 210.73049927  bias: 8.09891224\n",
      "epoch: 414  loss: 2048.54150391  weight: 210.73036194  bias: 8.10336685\n",
      "epoch: 415  loss: 2048.33203125  weight: 210.73022461  bias: 8.10782146\n",
      "epoch: 416  loss: 2048.13989258  weight: 210.73010254  bias: 8.11227608\n",
      "epoch: 417  loss: 2047.95153809  weight: 210.72996521  bias: 8.11672974\n",
      "epoch: 418  loss: 2047.74365234  weight: 210.72982788  bias: 8.12118340\n",
      "epoch: 419  loss: 2047.55017090  weight: 210.72970581  bias: 8.12563705\n",
      "epoch: 420  loss: 2047.34606934  weight: 210.72956848  bias: 8.13009071\n",
      "epoch: 421  loss: 2047.15185547  weight: 210.72944641  bias: 8.13454437\n",
      "epoch: 422  loss: 2046.94592285  weight: 210.72930908  bias: 8.13899708\n",
      "epoch: 423  loss: 2046.75097656  weight: 210.72917175  bias: 8.14344978\n",
      "epoch: 424  loss: 2046.54492188  weight: 210.72903442  bias: 8.14790249\n",
      "epoch: 425  loss: 2046.35437012  weight: 210.72891235  bias: 8.15235519\n",
      "epoch: 426  loss: 2046.16137695  weight: 210.72877502  bias: 8.15680695\n",
      "epoch: 427  loss: 2045.95471191  weight: 210.72863770  bias: 8.16125870\n",
      "epoch: 428  loss: 2045.76330566  weight: 210.72851562  bias: 8.16571045\n",
      "epoch: 429  loss: 2045.56030273  weight: 210.72837830  bias: 8.17016220\n",
      "epoch: 430  loss: 2045.36755371  weight: 210.72825623  bias: 8.17461300\n",
      "epoch: 431  loss: 2045.16064453  weight: 210.72811890  bias: 8.17906380\n",
      "epoch: 432  loss: 2044.96520996  weight: 210.72798157  bias: 8.18351460\n",
      "epoch: 433  loss: 2044.76025391  weight: 210.72784424  bias: 8.18796539\n",
      "epoch: 434  loss: 2044.56958008  weight: 210.72772217  bias: 8.19241619\n",
      "epoch: 435  loss: 2044.37792969  weight: 210.72758484  bias: 8.19686604\n",
      "epoch: 436  loss: 2044.17089844  weight: 210.72744751  bias: 8.20131588\n",
      "epoch: 437  loss: 2043.97961426  weight: 210.72732544  bias: 8.20576572\n",
      "epoch: 438  loss: 2043.77062988  weight: 210.72718811  bias: 8.21021557\n",
      "epoch: 439  loss: 2043.57995605  weight: 210.72705078  bias: 8.21466446\n",
      "epoch: 440  loss: 2043.37792969  weight: 210.72691345  bias: 8.21911335\n",
      "epoch: 441  loss: 2043.18298340  weight: 210.72679138  bias: 8.22356224\n",
      "epoch: 442  loss: 2042.97729492  weight: 210.72665405  bias: 8.22801113\n",
      "epoch: 443  loss: 2042.78906250  weight: 210.72653198  bias: 8.23246002\n",
      "epoch: 444  loss: 2042.59692383  weight: 210.72639465  bias: 8.23690796\n",
      "epoch: 445  loss: 2042.38818359  weight: 210.72625732  bias: 8.24135590\n",
      "epoch: 446  loss: 2042.19702148  weight: 210.72613525  bias: 8.24580383\n",
      "epoch: 447  loss: 2041.98852539  weight: 210.72599792  bias: 8.25025177\n",
      "epoch: 448  loss: 2041.80029297  weight: 210.72586060  bias: 8.25469875\n",
      "epoch: 449  loss: 2041.59399414  weight: 210.72572327  bias: 8.25914574\n",
      "epoch: 450  loss: 2041.40173340  weight: 210.72560120  bias: 8.26359272\n",
      "epoch: 451  loss: 2041.19494629  weight: 210.72546387  bias: 8.26803970\n",
      "epoch: 452  loss: 2041.00659180  weight: 210.72534180  bias: 8.27248669\n",
      "epoch: 453  loss: 2040.81445312  weight: 210.72520447  bias: 8.27693272\n",
      "epoch: 454  loss: 2040.60607910  weight: 210.72506714  bias: 8.28137875\n",
      "epoch: 455  loss: 2040.41577148  weight: 210.72494507  bias: 8.28582478\n",
      "epoch: 456  loss: 2040.21215820  weight: 210.72480774  bias: 8.29027081\n",
      "epoch: 457  loss: 2040.01940918  weight: 210.72468567  bias: 8.29471588\n",
      "epoch: 458  loss: 2039.81237793  weight: 210.72454834  bias: 8.29916096\n",
      "epoch: 459  loss: 2039.61877441  weight: 210.72441101  bias: 8.30360603\n",
      "epoch: 460  loss: 2039.41528320  weight: 210.72427368  bias: 8.30805111\n",
      "epoch: 461  loss: 2039.22497559  weight: 210.72415161  bias: 8.31249619\n",
      "epoch: 462  loss: 2039.03564453  weight: 210.72401428  bias: 8.31694031\n",
      "epoch: 463  loss: 2038.82983398  weight: 210.72387695  bias: 8.32138443\n",
      "epoch: 464  loss: 2038.63647461  weight: 210.72375488  bias: 8.32582855\n",
      "epoch: 465  loss: 2038.42993164  weight: 210.72361755  bias: 8.33027267\n",
      "epoch: 466  loss: 2038.24279785  weight: 210.72349548  bias: 8.33471584\n",
      "epoch: 467  loss: 2038.03381348  weight: 210.72335815  bias: 8.33915901\n",
      "epoch: 468  loss: 2037.84411621  weight: 210.72322083  bias: 8.34360218\n",
      "epoch: 469  loss: 2037.63574219  weight: 210.72308350  bias: 8.34804535\n",
      "epoch: 470  loss: 2037.44482422  weight: 210.72296143  bias: 8.35248756\n",
      "epoch: 471  loss: 2037.24133301  weight: 210.72282410  bias: 8.35692978\n",
      "epoch: 472  loss: 2037.05090332  weight: 210.72270203  bias: 8.36137199\n",
      "epoch: 473  loss: 2036.86145020  weight: 210.72256470  bias: 8.36581421\n",
      "epoch: 474  loss: 2036.65307617  weight: 210.72242737  bias: 8.37025642\n",
      "epoch: 475  loss: 2036.46203613  weight: 210.72230530  bias: 8.37469769\n",
      "epoch: 476  loss: 2036.26025391  weight: 210.72216797  bias: 8.37913895\n",
      "epoch: 477  loss: 2036.06616211  weight: 210.72204590  bias: 8.38358021\n",
      "epoch: 478  loss: 2035.86254883  weight: 210.72190857  bias: 8.38802147\n",
      "epoch: 479  loss: 2035.66784668  weight: 210.72177124  bias: 8.39246178\n",
      "epoch: 480  loss: 2035.46630859  weight: 210.72163391  bias: 8.39690208\n",
      "epoch: 481  loss: 2035.27429199  weight: 210.72151184  bias: 8.40134239\n",
      "epoch: 482  loss: 2035.08508301  weight: 210.72137451  bias: 8.40578270\n",
      "epoch: 483  loss: 2034.88073730  weight: 210.72123718  bias: 8.41022205\n",
      "epoch: 484  loss: 2034.69079590  weight: 210.72111511  bias: 8.41466141\n",
      "epoch: 485  loss: 2034.48413086  weight: 210.72097778  bias: 8.41910076\n",
      "epoch: 486  loss: 2034.29345703  weight: 210.72085571  bias: 8.42354012\n",
      "epoch: 487  loss: 2034.08703613  weight: 210.72071838  bias: 8.42797947\n",
      "epoch: 488  loss: 2033.89648438  weight: 210.72058105  bias: 8.43241787\n",
      "epoch: 489  loss: 2033.69421387  weight: 210.72044373  bias: 8.43685627\n",
      "epoch: 490  loss: 2033.49780273  weight: 210.72032166  bias: 8.44129467\n",
      "epoch: 491  loss: 2033.29492188  weight: 210.72018433  bias: 8.44573307\n",
      "epoch: 492  loss: 2033.10327148  weight: 210.72006226  bias: 8.45017147\n",
      "epoch: 493  loss: 2032.91601562  weight: 210.71992493  bias: 8.45460892\n",
      "epoch: 494  loss: 2032.71240234  weight: 210.71978760  bias: 8.45904636\n",
      "epoch: 495  loss: 2032.52136230  weight: 210.71966553  bias: 8.46348381\n",
      "epoch: 496  loss: 2032.31481934  weight: 210.71952820  bias: 8.46792126\n",
      "epoch: 497  loss: 2032.12536621  weight: 210.71940613  bias: 8.47235775\n",
      "epoch: 498  loss: 2031.92175293  weight: 210.71926880  bias: 8.47679424\n",
      "epoch: 499  loss: 2031.72778320  weight: 210.71914673  bias: 8.48123074\n",
      "epoch: 500  loss: 2031.52172852  weight: 210.71900940  bias: 8.48566723\n"
     ]
    }
   ],
   "source": [
    "X = torch.linspace(1,50,50).reshape(-1,1)\n",
    "torch.manual_seed(71) # to obtain reproducible results\n",
    "e = torch.randint(-1,1,(50,1),dtype=torch.float)\n",
    "y = 208*X + 100.0 \n",
    "\n",
    "model = Model(1,1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.0001)\n",
    "epochs = 500\n",
    "losses = []\n",
    "\n",
    "for i in range(epochs):\n",
    "    i+=1\n",
    "    y_pred = model.forward(X)\n",
    "    loss = criterion(y_pred, y)\n",
    "    losses.append(loss)\n",
    "    print(f'epoch: {i:<3}  loss: {loss.item():<10.8f}  weight: {model.linear.weight.item():10.8f}  \\\n",
    "bias: {model.linear.bias.item():10.8f}') \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
